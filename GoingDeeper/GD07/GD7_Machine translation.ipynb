{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a29721c",
   "metadata": {},
   "source": [
    "# Going Deeper 07 Machine translation by LSTM\n",
    "###### 온라인 2기 코어 박수경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec8d0d",
   "metadata": {},
   "source": [
    "## 라이브러리 및 데이터 로딩  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ccb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abae9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30749</th>\n",
       "      <td>Don't believe everything people tell you.</td>\n",
       "      <td>Ne croyez pas à tout ce que les gens vous disent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20014</th>\n",
       "      <td>He just texted me.</td>\n",
       "      <td>Il vient de me faire parvenir un texto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23864</th>\n",
       "      <td>Did you go out?</td>\n",
       "      <td>Es-tu sortie ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22111</th>\n",
       "      <td>I'm the one who killed Tom.</td>\n",
       "      <td>Je suis celui qui a tué Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18342</th>\n",
       "      <td>Don't turn off your computer.</td>\n",
       "      <td>N'éteins pas ton ordinateur.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eng  \\\n",
       "30749  Don't believe everything people tell you.   \n",
       "20014                         He just texted me.   \n",
       "23864                            Did you go out?   \n",
       "22111                I'm the one who killed Tom.   \n",
       "18342              Don't turn off your computer.   \n",
       "\n",
       "                                                     fra  \n",
       "30749  Ne croyez pas à tout ce que les gens vous disent.  \n",
       "20014            Il vient de me faire parvenir un texto.  \n",
       "23864                                     Es-tu sortie ?  \n",
       "22111                       Je suis celui qui a tué Tom.  \n",
       "18342                       N'éteins pas ton ordinateur.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "\n",
    "lines = lines.sample(frac=1).reset_index(drop=True) # 섞고 인덱스 리셋하기 -섞지않은 데이터는 문자열순서대로 정렬되어 있으므로.\n",
    "\n",
    "lines = lines[['eng', 'fra']][:33000] # 사용하는 샘플 개수\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75594369",
   "metadata": {},
   "source": [
    "seq2seq 번역모델은 인코더, 디코더 두 부분으로 나뉘어져 있다.  \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/E-15-4.seq2seq.max-800x600.jpg)\n",
    "인코더에서 사용하는 시퀀스와는 달리 디코더는 시퀀스의 처음과 끝에 각각을 알리는 토큰이 필요하다. \n",
    "기본적인 정제를 다 거치고 30000개는 학습 셋, 3000개는 테스트 셋으로 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f82546",
   "metadata": {},
   "source": [
    "## 정제, 정규화 및 기타 전처리  \n",
    "\n",
    "글자 단위가 아닌 단어 단위의 번역기를 하기 위해서 전처리를 한다.  \n",
    "Punctuation을단어와 분리하고, 대문자를 모두 소문자로 바꾼다. 그 후 띄어쓰기 단위로 문장을 토큰화한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0ce0a",
   "metadata": {},
   "source": [
    "필요한 전처리 과정을 함수화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48eecd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe897f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f189c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<s>'\n",
    "end_token = '<e>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1946478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 띄어쓰기를 기준으로 단어들을 분리\n",
    "# 2. 구두점을 단어와 분리 (문장의 끝을 알리는 문장부호)\n",
    "# 3. 모두 소문자화하기 * 궁금한 점. 고유명사는 어떻게???\n",
    "\n",
    "def polish_up(lines, token = False ):\n",
    "        \n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = str(lines[i]).lower()# 소문자\n",
    "        lines[i] = ''.join(char for char in lines[i] if char not in punctuation) # 구두점(Punctuation)을 단어와 분리하기\n",
    "          \n",
    "        \n",
    "    if token == True:\n",
    "        for i in range(len(lines)):\n",
    "            lines[i] = start_token +' '+ lines[i] +' '+ end_token     \n",
    "    \n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dab2ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <s> je ne savais pas quelles pouvaient faire ç...\n",
       "1          <s> voulezvous que jespionne tom pour vous  <e>\n",
       "2        <s> je dois retoucher mes vêtements parce que ...\n",
       "3        <s> «entre seulement et dis au patron que tu v...\n",
       "4                <s> il sarrêta pour regarder laffiche <e>\n",
       "                               ...                        \n",
       "32995    <s> les enfants ont parfois peur dans le noir <e>\n",
       "32996    <s> puisje aller vous chercher quoi que ce soi...\n",
       "32997                 <s> la vieille femme est médecin <e>\n",
       "32998                  <s> nous avons bu toute la nuit <e>\n",
       "32999          <s> vous avez atteint votre destination <e>\n",
       "Name: fra, Length: 33000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polish_up(lines.eng)\n",
    "polish_up(lines.fra, token = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083b7942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7022     <s> je ne pense pas quaucun de nous deux ne ve...\n",
       "20006    <s> javais hâte de voir une vue panoramique du...\n",
       "25457                 <s> je nai rien dautre à ajouter <e>\n",
       "22709    <s> je suis habituée à ce que tom me crie tout...\n",
       "29508    <s> je te promets que jexpliquerai tout plus t...\n",
       "10251         <s> pourquoi ne discutestu pas avec lui  <e>\n",
       "17376                <s> il est difficile dêtre parent <e>\n",
       "6784                             <s> lâchezmoi le bras <e>\n",
       "25582             <s> je sais que ça doit être un choc <e>\n",
       "2868                   <s> je sais que vous lavez fait <e>\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng.sample(10)\n",
    "lines.fra.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbfc95",
   "metadata": {},
   "source": [
    "## Tokenize  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dcc1e",
   "metadata": {},
   "source": [
    "\n",
    "### 텍스트를 토큰으로 인코딩  \n",
    "\n",
    "참고 : https://wikidocs.net/31766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ae942a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 55, 30, 42, 86, 9, 8],\n",
       " [9, 2, 27, 14, 3, 2525, 29, 7, 20, 2],\n",
       " [1, 16, 3, 2929, 19, 467, 298, 76, 285, 849]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=False)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f1bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943de2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 9, 289, 5, 413, 2815, 27, 29, 2],\n",
       " [1, 251, 6, 7632, 12, 25, 11, 2],\n",
       " [1, 3, 124, 7633, 95, 529, 556, 6, 23, 285, 37, 1018, 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a38889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰 제거\n",
    "\n",
    "decoder_input = [[char for char in line if char != 1] for line in target_text]\n",
    "decoder_target =[[char for char in line if char != 2] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710398bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 8346\n",
      "프랑스어 단어장의 크기 : 15969\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3bdf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f9cd8",
   "metadata": {},
   "source": [
    "## Embedding layer (각 단어를 임베딩 층을 사용하여 벡터화)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a34317",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e068e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "encoder_inputs = Input(shape=(None,), name='incoder_input')\n",
    "enc_emb =  Embedding(eng_vocab_size, emb_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden_size , return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d38b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, emb_dim)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)\n",
    "\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960d8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_seq_len = 11\n",
    "max_fra_seq_len = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d4b7e",
   "metadata": {},
   "source": [
    "## 모델링  \n",
    "\n",
    "label이 integer 값이므로 categorical entropy loss가 아닌 sparse categorical entropy loss를 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d88608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "incoder_input (InputLayer)   [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         2136576   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 512), (None, 512) 1574912   \n",
      "=================================================================\n",
      "Total params: 3,711,488\n",
      "Trainable params: 3,711,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb31e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(512,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(512,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "dec_emb_2 = Embedding(fra_vocab_size, emb_dim)(decoder_inputs)\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dec_emb_2, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb2afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    4088064     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1574912     embedding_3[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 15969)  8192097     lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 13,855,073\n",
      "Trainable params: 13,855,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#디코더의 출력층 재설계\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50a708",
   "metadata": {},
   "source": [
    "## Model evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3201a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer.word_index['<s>'] = 1\n",
    "fra_tokenizer.word_index['<e>'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_to_idx = eng_tokenizer.word_index\n",
    "fra_to_idx = fra_tokenizer.word_index\n",
    "idx_to_ng = eng_tokenizer.index_word\n",
    "idx_to_fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c46413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer.index_word[1] = '<s>'\n",
    "fra_tokenizer.index_word[2] = '<e>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "549bb013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<s>',\n",
       " 2: '<e>',\n",
       " 3: 'je',\n",
       " 4: 'de',\n",
       " 5: 'pas',\n",
       " 6: 'que',\n",
       " 7: 'à',\n",
       " 8: 'la',\n",
       " 9: 'ne',\n",
       " 10: 'le',\n",
       " 11: 'vous',\n",
       " 12: 'tom',\n",
       " 13: 'il',\n",
       " 14: 'est',\n",
       " 15: 'un',\n",
       " 16: 'ce',\n",
       " 17: 'a',\n",
       " 18: 'tu',\n",
       " 19: 'nous',\n",
       " 20: 'en',\n",
       " 21: 'les',\n",
       " 22: 'une',\n",
       " 23: 'jai',\n",
       " 24: 'suis',\n",
       " 25: 'pour',\n",
       " 26: 'me',\n",
       " 27: 'faire',\n",
       " 28: 'cest',\n",
       " 29: 'ça',\n",
       " 30: 'des',\n",
       " 31: 'plus',\n",
       " 32: 'elle',\n",
       " 33: 'dans',\n",
       " 34: 'tout',\n",
       " 35: 'qui',\n",
       " 36: 'te',\n",
       " 37: 'du',\n",
       " 38: 'ma',\n",
       " 39: 'se',\n",
       " 40: 'au',\n",
       " 41: 'fait',\n",
       " 42: 'avec',\n",
       " 43: 'si',\n",
       " 44: 'mon',\n",
       " 45: 'veux',\n",
       " 46: 'et',\n",
       " 47: 'sont',\n",
       " 48: 'quil',\n",
       " 49: 'y',\n",
       " 50: 'cette',\n",
       " 51: 'très',\n",
       " 52: 'votre',\n",
       " 53: 'pense',\n",
       " 54: 'être',\n",
       " 55: 'cela',\n",
       " 56: 'son',\n",
       " 57: 'nest',\n",
       " 58: 'sur',\n",
       " 59: 'été',\n",
       " 60: 'pourquoi',\n",
       " 61: 'temps',\n",
       " 62: 'dit',\n",
       " 63: 'moi',\n",
       " 64: 'était',\n",
       " 65: 'peux',\n",
       " 66: 'ils',\n",
       " 67: 'lui',\n",
       " 68: 'ici',\n",
       " 69: 'chose',\n",
       " 70: 'nai',\n",
       " 71: 'sais',\n",
       " 72: 'jamais',\n",
       " 73: 'où',\n",
       " 74: 'estce',\n",
       " 75: 'comment',\n",
       " 76: 'quelque',\n",
       " 77: 'ton',\n",
       " 78: 'toi',\n",
       " 79: 'besoin',\n",
       " 80: 'bien',\n",
       " 81: 'vraiment',\n",
       " 82: 'dire',\n",
       " 83: 'beaucoup',\n",
       " 84: 'on',\n",
       " 85: 'par',\n",
       " 86: 'êtes',\n",
       " 87: 'ont',\n",
       " 88: 'sa',\n",
       " 89: 'personne',\n",
       " 90: 'tous',\n",
       " 91: 'avons',\n",
       " 92: 'toujours',\n",
       " 93: 'quelle',\n",
       " 94: 'sest',\n",
       " 95: 'mes',\n",
       " 96: 'peu',\n",
       " 97: 'comme',\n",
       " 98: 'va',\n",
       " 99: 'sommes',\n",
       " 100: 'astu',\n",
       " 101: 'es',\n",
       " 102: 'ta',\n",
       " 103: 'monde',\n",
       " 104: 'as',\n",
       " 105: 'aller',\n",
       " 106: 'parler',\n",
       " 107: 'quand',\n",
       " 108: 'elles',\n",
       " 109: 'rien',\n",
       " 110: 'encore',\n",
       " 111: 'maison',\n",
       " 112: 'êtesvous',\n",
       " 113: 'sil',\n",
       " 114: 'avoir',\n",
       " 115: 'deux',\n",
       " 116: 'train',\n",
       " 117: 'trop',\n",
       " 118: 'quoi',\n",
       " 119: 'voir',\n",
       " 120: 'vais',\n",
       " 121: 'avezvous',\n",
       " 122: 'bon',\n",
       " 123: 'mary',\n",
       " 124: 'dois',\n",
       " 125: 'soit',\n",
       " 126: 'na',\n",
       " 127: 'aussi',\n",
       " 128: 'combien',\n",
       " 129: 'mais',\n",
       " 130: 'ses',\n",
       " 131: 'jaimerais',\n",
       " 132: 'ai',\n",
       " 133: 'là',\n",
       " 134: 'avez',\n",
       " 135: 'peut',\n",
       " 136: 'français',\n",
       " 137: 'voiture',\n",
       " 138: 'chez',\n",
       " 139: 'lair',\n",
       " 140: 'faut',\n",
       " 141: 'vu',\n",
       " 142: 'même',\n",
       " 143: 'déjà',\n",
       " 144: 'maintenant',\n",
       " 145: 'travail',\n",
       " 146: 'tes',\n",
       " 147: 'juste',\n",
       " 148: 'aujourdhui',\n",
       " 149: 'fois',\n",
       " 150: 'jétais',\n",
       " 151: 'ces',\n",
       " 152: 'quelquun',\n",
       " 153: 'avait',\n",
       " 154: 'quel',\n",
       " 155: 'eu',\n",
       " 156: 'cétait',\n",
       " 157: 'marie',\n",
       " 158: 'ny',\n",
       " 159: 'demain',\n",
       " 160: 'ceci',\n",
       " 161: 'pensais',\n",
       " 162: 'dun',\n",
       " 163: 'notre',\n",
       " 164: 'père',\n",
       " 165: 'fais',\n",
       " 166: 'estu',\n",
       " 167: 'toute',\n",
       " 168: 'noël',\n",
       " 169: 'prendre',\n",
       " 170: 'plaît',\n",
       " 171: 'hier',\n",
       " 172: 'assez',\n",
       " 173: 'manger',\n",
       " 174: 'sans',\n",
       " 175: 'bonne',\n",
       " 176: 'mal',\n",
       " 177: 'mieux',\n",
       " 178: 'trois',\n",
       " 179: 'atil',\n",
       " 180: 'livre',\n",
       " 181: 'toutes',\n",
       " 182: 'puisje',\n",
       " 183: 'vos',\n",
       " 184: 'dêtre',\n",
       " 185: 'savoir',\n",
       " 186: 'gens',\n",
       " 187: 'nestce',\n",
       " 188: 'seul',\n",
       " 189: 'porte',\n",
       " 190: 'heures',\n",
       " 191: 'dû',\n",
       " 192: 'avant',\n",
       " 193: 'soir',\n",
       " 194: 'aux',\n",
       " 195: 'vie',\n",
       " 196: 'nuit',\n",
       " 197: 'quelques',\n",
       " 198: 'enfants',\n",
       " 199: 'jour',\n",
       " 200: 'ans',\n",
       " 201: 'problème',\n",
       " 202: 'mère',\n",
       " 203: 'cet',\n",
       " 204: 'moment',\n",
       " 205: 'dune',\n",
       " 206: 'chien',\n",
       " 207: 'questce',\n",
       " 208: 'passé',\n",
       " 209: 'partir',\n",
       " 210: 'choses',\n",
       " 211: 'ou',\n",
       " 212: 'depuis',\n",
       " 213: 'sens',\n",
       " 214: 'lai',\n",
       " 215: 'narrive',\n",
       " 216: 'jaime',\n",
       " 217: 'heureux',\n",
       " 218: 'tôt',\n",
       " 219: 'pouvez',\n",
       " 220: 'sûr',\n",
       " 221: 'après',\n",
       " 222: 'devrais',\n",
       " 223: 'rester',\n",
       " 224: 'dernière',\n",
       " 225: 'voulais',\n",
       " 226: 'moins',\n",
       " 227: 'veut',\n",
       " 228: 'pris',\n",
       " 229: 'souvent',\n",
       " 230: 'semble',\n",
       " 231: 'jours',\n",
       " 232: 'longtemps',\n",
       " 233: 'dargent',\n",
       " 234: 'nouveau',\n",
       " 235: 'fut',\n",
       " 236: 'pendant',\n",
       " 237: 'lécole',\n",
       " 238: 'boston',\n",
       " 239: 'semaine',\n",
       " 240: 'croire',\n",
       " 241: 'entendu',\n",
       " 242: 'peur',\n",
       " 243: 'sait',\n",
       " 244: 'sois',\n",
       " 245: 'rendre',\n",
       " 246: 'leur',\n",
       " 247: 'grand',\n",
       " 248: 'simplement',\n",
       " 249: 'part',\n",
       " 250: 'lire',\n",
       " 251: 'voulezvous',\n",
       " 252: 'venir',\n",
       " 253: 'crois',\n",
       " 254: 'largent',\n",
       " 255: 'pu',\n",
       " 256: 'jespère',\n",
       " 257: 'nom',\n",
       " 258: 'fille',\n",
       " 259: 'idée',\n",
       " 260: 'seule',\n",
       " 261: 'raison',\n",
       " 262: 'devons',\n",
       " 263: 'demandé',\n",
       " 264: 'autre',\n",
       " 265: 'amis',\n",
       " 266: 'parle',\n",
       " 267: 'trouver',\n",
       " 268: 'soyez',\n",
       " 269: 'difficile',\n",
       " 270: 'acheté',\n",
       " 271: 'matin',\n",
       " 272: 'jusquà',\n",
       " 273: 'veuxtu',\n",
       " 274: 'femme',\n",
       " 275: 'trouvé',\n",
       " 276: 'passer',\n",
       " 277: 'pouvezvous',\n",
       " 278: 'retard',\n",
       " 279: 'quon',\n",
       " 280: 'prie',\n",
       " 281: 'journée',\n",
       " 282: 'arrivé',\n",
       " 283: 'demande',\n",
       " 284: 'heure',\n",
       " 285: 'perdu',\n",
       " 286: 'lorsque',\n",
       " 287: 'sera',\n",
       " 288: 'aucun',\n",
       " 289: 'savais',\n",
       " 290: 'faites',\n",
       " 291: 'chambre',\n",
       " 292: 'javais',\n",
       " 293: 'peuxtu',\n",
       " 294: 'café',\n",
       " 295: 'merci',\n",
       " 296: 'aider',\n",
       " 297: 'reste',\n",
       " 298: 'dautre',\n",
       " 299: 'nétait',\n",
       " 300: 'pouvons',\n",
       " 301: 'non',\n",
       " 302: 'homme',\n",
       " 303: 'ami',\n",
       " 304: 'tard',\n",
       " 305: 'nêtes',\n",
       " 306: 'bureau',\n",
       " 307: 'naime',\n",
       " 308: 'désolé',\n",
       " 309: 'nen',\n",
       " 310: 'chaque',\n",
       " 311: 'presque',\n",
       " 312: 'doit',\n",
       " 313: 'allé',\n",
       " 314: 'question',\n",
       " 315: 'mort',\n",
       " 316: 'tai',\n",
       " 317: 'trouve',\n",
       " 318: 'ville',\n",
       " 319: 'petit',\n",
       " 320: 'vrai',\n",
       " 321: 'attendre',\n",
       " 322: 'devriez',\n",
       " 323: 'jeune',\n",
       " 324: 'parents',\n",
       " 325: 'peutêtre',\n",
       " 326: 'pièce',\n",
       " 327: 'veuillez',\n",
       " 328: 'partie',\n",
       " 329: 'aucune',\n",
       " 330: 'famille',\n",
       " 331: 'pourrait',\n",
       " 332: 'lettre',\n",
       " 333: 'lit',\n",
       " 334: 'main',\n",
       " 335: 'vite',\n",
       " 336: 'vérité',\n",
       " 337: 'nouvelle',\n",
       " 338: 'pensé',\n",
       " 339: 'dis',\n",
       " 340: 'jouer',\n",
       " 341: 'étaient',\n",
       " 342: 'près',\n",
       " 343: 'facile',\n",
       " 344: 'réunion',\n",
       " 345: 'davantage',\n",
       " 346: 'quils',\n",
       " 347: 'connais',\n",
       " 348: 'passe',\n",
       " 349: 'venu',\n",
       " 350: 'mis',\n",
       " 351: 'ensemble',\n",
       " 352: 'daccord',\n",
       " 353: 'estil',\n",
       " 354: 'sujet',\n",
       " 355: 'donné',\n",
       " 356: 'viens',\n",
       " 357: 'devez',\n",
       " 358: 'acheter',\n",
       " 359: 'laisser',\n",
       " 360: 'tellement',\n",
       " 361: 'davoir',\n",
       " 362: 'petite',\n",
       " 363: 'nes',\n",
       " 364: 'vieux',\n",
       " 365: 'photo',\n",
       " 366: 'pouvoir',\n",
       " 367: 'seulement',\n",
       " 368: 'téléphone',\n",
       " 369: 'grande',\n",
       " 370: 'fort',\n",
       " 371: 'occupé',\n",
       " 372: 'aime',\n",
       " 373: 'frère',\n",
       " 374: 'dont',\n",
       " 375: 'tête',\n",
       " 376: 'probablement',\n",
       " 377: 'voulez',\n",
       " 378: 'sortir',\n",
       " 379: 'belle',\n",
       " 380: 'chat',\n",
       " 381: 'mangé',\n",
       " 382: 'personnes',\n",
       " 383: 'bientôt',\n",
       " 384: 'voulait',\n",
       " 385: 'jen',\n",
       " 386: 'alors',\n",
       " 387: 'lheure',\n",
       " 388: 'travailler',\n",
       " 389: 'point',\n",
       " 390: 'enfant',\n",
       " 391: 'colère',\n",
       " 392: 'questions',\n",
       " 393: 'yeux',\n",
       " 394: 'allons',\n",
       " 395: 'sœur',\n",
       " 396: 'nager',\n",
       " 397: 'place',\n",
       " 398: 'men',\n",
       " 399: 'oublié',\n",
       " 400: 'malade',\n",
       " 401: 'dernier',\n",
       " 402: 'côté',\n",
       " 403: 'contre',\n",
       " 404: 'pensestu',\n",
       " 405: 'vivre',\n",
       " 406: 'envie',\n",
       " 407: 'froid',\n",
       " 408: 'nos',\n",
       " 409: 'essayé',\n",
       " 410: 'dici',\n",
       " 411: 'bus',\n",
       " 412: 'vient',\n",
       " 413: 'quelles',\n",
       " 414: 'table',\n",
       " 415: 'fini',\n",
       " 416: 'confiance',\n",
       " 417: 'travaille',\n",
       " 418: 'dur',\n",
       " 419: 'pouvais',\n",
       " 420: 'autant',\n",
       " 421: 'fête',\n",
       " 422: 'chance',\n",
       " 423: 'livres',\n",
       " 424: 'pourrais',\n",
       " 425: 'mot',\n",
       " 426: 'donner',\n",
       " 427: 'devrait',\n",
       " 428: 'prêt',\n",
       " 429: 'vue',\n",
       " 430: 'leau',\n",
       " 431: 'vélo',\n",
       " 432: 'dix',\n",
       " 433: 'cours',\n",
       " 434: 'apprendre',\n",
       " 435: 'lautre',\n",
       " 436: 'serai',\n",
       " 437: 'nas',\n",
       " 438: 'comprends',\n",
       " 439: 'déteste',\n",
       " 440: 'heureuse',\n",
       " 441: 'décidé',\n",
       " 442: 'minutes',\n",
       " 443: 'étiez',\n",
       " 444: '«',\n",
       " 445: 'propos',\n",
       " 446: 'meilleur',\n",
       " 447: 'pays',\n",
       " 448: 'étais',\n",
       " 449: 'déjeuner',\n",
       " 450: '»',\n",
       " 451: 'police',\n",
       " 452: 'dîner',\n",
       " 453: 'sous',\n",
       " 454: 'problèmes',\n",
       " 455: 'vois',\n",
       " 456: 'jaurais',\n",
       " 457: 'réponse',\n",
       " 458: 'mois',\n",
       " 459: 'appris',\n",
       " 460: 'pensezvous',\n",
       " 461: 'serais',\n",
       " 462: 'comprendre',\n",
       " 463: 'parlé',\n",
       " 464: 'coup',\n",
       " 465: 'prends',\n",
       " 466: 'ça\\xa0',\n",
       " 467: 'regarder',\n",
       " 468: 'exactement',\n",
       " 469: 'erreur',\n",
       " 470: 'sûre',\n",
       " 471: 'attention',\n",
       " 472: 'daller',\n",
       " 473: 'aide',\n",
       " 474: 'leurs',\n",
       " 475: 'joue',\n",
       " 476: 'tant',\n",
       " 477: 'chaud',\n",
       " 478: 'dehors',\n",
       " 479: 'gros',\n",
       " 480: 'commencé',\n",
       " 481: 'devoirs',\n",
       " 482: 'payer',\n",
       " 483: 'jignore',\n",
       " 484: 'voyage',\n",
       " 485: 'médecin',\n",
       " 486: 'feu',\n",
       " 487: 'plutôt',\n",
       " 488: 'parti',\n",
       " 489: 'ferais',\n",
       " 490: 'premier',\n",
       " 491: 'possible',\n",
       " 492: 'histoire',\n",
       " 493: 'jadore',\n",
       " 494: 'chemin',\n",
       " 495: 'rencontrer',\n",
       " 496: 'garçon',\n",
       " 497: 'prix',\n",
       " 498: 'compte',\n",
       " 499: 'écrit',\n",
       " 500: 'conduire',\n",
       " 501: 'secret',\n",
       " 502: 'derrière',\n",
       " 503: 'autres',\n",
       " 504: 'peine',\n",
       " 505: 'amie',\n",
       " 506: 'regarde',\n",
       " 507: 'fils',\n",
       " 508: 'aije',\n",
       " 509: 'chaussures',\n",
       " 510: 'laissé',\n",
       " 511: 'voudrais',\n",
       " 512: 'manière',\n",
       " 513: 'première',\n",
       " 514: 'auparavant',\n",
       " 515: 'beau',\n",
       " 516: 'mauvais',\n",
       " 517: 'surpris',\n",
       " 518: 'celui',\n",
       " 519: 'laisse',\n",
       " 520: 'maider',\n",
       " 521: 'désolée',\n",
       " 522: 'entre',\n",
       " 523: 'cheveux',\n",
       " 524: 'dormir',\n",
       " 525: 'gare',\n",
       " 526: 'musique',\n",
       " 527: 'navons',\n",
       " 528: 'faim',\n",
       " 529: 'vêtements',\n",
       " 530: 'tennis',\n",
       " 531: 'endroit',\n",
       " 532: 'immédiatement',\n",
       " 533: 'plan',\n",
       " 534: 'fatigué',\n",
       " 535: 'vit',\n",
       " 536: 'pourriezvous',\n",
       " 537: 'aprèsmidi',\n",
       " 538: 'boulot',\n",
       " 539: 'répondre',\n",
       " 540: 'plein',\n",
       " 541: 'navais',\n",
       " 542: 'serait',\n",
       " 543: 'content',\n",
       " 544: 'fenêtre',\n",
       " 545: 'vers',\n",
       " 546: 'arrive',\n",
       " 547: 'film',\n",
       " 548: 'taider',\n",
       " 549: 'prochaine',\n",
       " 550: 'penser',\n",
       " 551: 'tort',\n",
       " 552: 'demander',\n",
       " 553: 'route',\n",
       " 554: 'nourriture',\n",
       " 555: 'laissezmoi',\n",
       " 556: 'parce',\n",
       " 557: 'jy',\n",
       " 558: 'devoir',\n",
       " 559: 'argent',\n",
       " 560: 'saistu',\n",
       " 561: 'blessé',\n",
       " 562: 'resté',\n",
       " 563: 'faisait',\n",
       " 564: 'marché',\n",
       " 565: 'lieu',\n",
       " 566: 'professeur',\n",
       " 567: 'choix',\n",
       " 568: 'visite',\n",
       " 569: 'terminé',\n",
       " 570: 'allez',\n",
       " 571: 'riche',\n",
       " 572: 'quiconque',\n",
       " 573: 'mettre',\n",
       " 574: 'puisse',\n",
       " 575: 'important',\n",
       " 576: 'thé',\n",
       " 577: 'ainsi',\n",
       " 578: 'parfois',\n",
       " 579: 'liste',\n",
       " 580: 'étions',\n",
       " 581: 'arrête',\n",
       " 582: 'marcher',\n",
       " 583: 'écrire',\n",
       " 584: 'fin',\n",
       " 585: 'devrions',\n",
       " 586: 'japon',\n",
       " 587: 'cœur',\n",
       " 588: 'cause',\n",
       " 589: 'année',\n",
       " 590: 'bois',\n",
       " 591: 'laissemoi',\n",
       " 592: 'lun',\n",
       " 593: 'ferai',\n",
       " 594: 'mains',\n",
       " 595: 'commence',\n",
       " 596: 'situation',\n",
       " 597: 'genre',\n",
       " 598: 'assis',\n",
       " 599: 'lu',\n",
       " 600: 'prenez',\n",
       " 601: 'rencontré',\n",
       " 602: 'anglais',\n",
       " 603: 'loin',\n",
       " 604: 'vont',\n",
       " 605: 'vaut',\n",
       " 606: 'savait',\n",
       " 607: 'propre',\n",
       " 608: 'rappelle',\n",
       " 609: 'lannée',\n",
       " 610: 'fumer',\n",
       " 611: 'soleil',\n",
       " 612: 'âge',\n",
       " 613: 'navez',\n",
       " 614: 'plait',\n",
       " 615: 'lhomme',\n",
       " 616: 'laccident',\n",
       " 617: 'décision',\n",
       " 618: 'rentrer',\n",
       " 619: 'vacances',\n",
       " 620: 'mas',\n",
       " 621: 'tasse',\n",
       " 622: 'sorte',\n",
       " 623: 'sen',\n",
       " 624: 'chanson',\n",
       " 625: 'essayer',\n",
       " 626: 'làbas',\n",
       " 627: 'savezvous',\n",
       " 628: 'discuter',\n",
       " 629: 'chiens',\n",
       " 630: 'navait',\n",
       " 631: 'perdre',\n",
       " 632: 'montre',\n",
       " 633: 'verre',\n",
       " 634: 'cadeau',\n",
       " 635: 'étudiants',\n",
       " 636: 'guerre',\n",
       " 637: 'clé',\n",
       " 638: 'devant',\n",
       " 639: 'vieille',\n",
       " 640: 'terre',\n",
       " 641: 'fit',\n",
       " 642: 'affaires',\n",
       " 643: 'garder',\n",
       " 644: 'nouvelles',\n",
       " 645: 'aimé',\n",
       " 646: 'boire',\n",
       " 647: 'chanter',\n",
       " 648: 'voulons',\n",
       " 649: 'bière',\n",
       " 650: 'entendre',\n",
       " 651: 'quune',\n",
       " 652: 'langue',\n",
       " 653: 'parc',\n",
       " 654: 'pluie',\n",
       " 655: 'dollars',\n",
       " 656: 'façon',\n",
       " 657: 'boîte',\n",
       " 658: 'rue',\n",
       " 659: 'bébé',\n",
       " 660: 'étudier',\n",
       " 661: 'nécessaire',\n",
       " 662: 'droit',\n",
       " 663: 'magasin',\n",
       " 664: 'long',\n",
       " 665: 'payé',\n",
       " 666: 'gâteau',\n",
       " 667: 'journal',\n",
       " 668: 'reçu',\n",
       " 669: 'lintention',\n",
       " 670: 'conseil',\n",
       " 671: 'prochain',\n",
       " 672: 'occupée',\n",
       " 673: 'senti',\n",
       " 674: 'sécurité',\n",
       " 675: 'pas\\xa0',\n",
       " 676: 'poser',\n",
       " 677: 'vouloir',\n",
       " 678: 'patron',\n",
       " 679: 'quun',\n",
       " 680: 'chercher',\n",
       " 681: 'santé',\n",
       " 682: 'plaisir',\n",
       " 683: 'dy',\n",
       " 684: 'changer',\n",
       " 685: 'appelé',\n",
       " 686: 'suite',\n",
       " 687: 'taxi',\n",
       " 688: 'souhaite',\n",
       " 689: 'lait',\n",
       " 690: 'j’ai',\n",
       " 691: 'laide',\n",
       " 692: 'gagner',\n",
       " 693: 'nimporte',\n",
       " 694: 'rivière',\n",
       " 695: 'mauvaise',\n",
       " 696: 'ten',\n",
       " 697: 'dès',\n",
       " 698: 'montrer',\n",
       " 699: 'veulent',\n",
       " 700: 'type',\n",
       " 701: 'revoir',\n",
       " 702: 'bruit',\n",
       " 703: 'quatre',\n",
       " 704: 'fasse',\n",
       " 705: 'cinq',\n",
       " 706: 'mets',\n",
       " 707: 'laquelle',\n",
       " 708: 'dites',\n",
       " 709: 'autour',\n",
       " 710: 'vas',\n",
       " 711: 'dispose',\n",
       " 712: 'pourraistu',\n",
       " 713: 'trente',\n",
       " 714: 'nombreux',\n",
       " 715: 'mange',\n",
       " 716: 'avais',\n",
       " 717: 'pouvait',\n",
       " 718: 'rapport',\n",
       " 719: 'pleuvoir',\n",
       " 720: 'vin',\n",
       " 721: 'complètement',\n",
       " 722: 'dormi',\n",
       " 723: 'rouge',\n",
       " 724: 'minute',\n",
       " 725: 'ait',\n",
       " 726: 'moimême',\n",
       " 727: 'règles',\n",
       " 728: 'naurais',\n",
       " 729: 'jardin',\n",
       " 730: 'c’est',\n",
       " 731: 'lune',\n",
       " 732: 'affaire',\n",
       " 733: 'lavezvous',\n",
       " 734: 'lundi',\n",
       " 735: 'fier',\n",
       " 736: 'faisons',\n",
       " 737: 'robe',\n",
       " 738: 'femmes',\n",
       " 739: 'donc',\n",
       " 740: 'connaît',\n",
       " 741: 'filles',\n",
       " 742: 'numéro',\n",
       " 743: 'clés',\n",
       " 744: 'suisje',\n",
       " 745: 'oncle',\n",
       " 746: 'rapidement',\n",
       " 747: 'dimanche',\n",
       " 748: 'porter',\n",
       " 749: 'neige',\n",
       " 750: 'amies',\n",
       " 751: 'rapide',\n",
       " 752: 'dautres',\n",
       " 753: 'devenu',\n",
       " 754: 'mesure',\n",
       " 755: 'chapeau',\n",
       " 756: 'arrêté',\n",
       " 757: 'plusieurs',\n",
       " 758: 'finalement',\n",
       " 759: 'arriver',\n",
       " 760: 'danser',\n",
       " 761: 'peuvent',\n",
       " 762: 'japprécie',\n",
       " 763: 'classe',\n",
       " 764: 'nombreuses',\n",
       " 765: 'telle',\n",
       " 766: 'seriez',\n",
       " 767: 'stupide',\n",
       " 768: 'dictionnaire',\n",
       " 769: 'prit',\n",
       " 770: 'désormais',\n",
       " 771: 'suppose',\n",
       " 772: 'plage',\n",
       " 773: 'vastu',\n",
       " 774: 'dirait',\n",
       " 775: 'arrêter',\n",
       " 776: 'vide',\n",
       " 777: 'cela\\xa0',\n",
       " 778: 'accident',\n",
       " 779: 'penses',\n",
       " 780: 'weekend',\n",
       " 781: 'dismoi',\n",
       " 782: 'années',\n",
       " 783: 'devenir',\n",
       " 784: 'équipe',\n",
       " 785: 'langlais',\n",
       " 786: 'parapluie',\n",
       " 787: 'produit',\n",
       " 788: 'bras',\n",
       " 789: 'avocat',\n",
       " 790: 'retour',\n",
       " 791: 'simple',\n",
       " 792: 'celle',\n",
       " 793: 'donnemoi',\n",
       " 794: 'vis',\n",
       " 795: 'mots',\n",
       " 796: 'utiliser',\n",
       " 797: 'commencer',\n",
       " 798: 'capable',\n",
       " 799: 'mari',\n",
       " 800: 'deau',\n",
       " 801: 'dangereux',\n",
       " 802: 'voudriezvous',\n",
       " 803: 'oui',\n",
       " 804: 'rêve',\n",
       " 805: 'regardé',\n",
       " 806: 'aidé',\n",
       " 807: 'tomber',\n",
       " 808: 'excuses',\n",
       " 809: 'meilleure',\n",
       " 810: 'télé',\n",
       " 811: 'moitié',\n",
       " 812: 'durant',\n",
       " 813: 'sagit',\n",
       " 814: '«\\xa0je',\n",
       " 815: 'pouvonsnous',\n",
       " 816: 'nont',\n",
       " 817: 'garde',\n",
       " 818: 'fermer',\n",
       " 819: 'six',\n",
       " 820: 'ressemble',\n",
       " 821: 'dessus',\n",
       " 822: 'télévision',\n",
       " 823: 'changé',\n",
       " 824: 'volé',\n",
       " 825: 'savons',\n",
       " 826: 'car',\n",
       " 827: 'fleurs',\n",
       " 828: 'animaux',\n",
       " 829: 'souviens',\n",
       " 830: 'réussi',\n",
       " 831: 'quitter',\n",
       " 832: 'gagné',\n",
       " 833: 'sérieux',\n",
       " 834: 'tableau',\n",
       " 835: 'risque',\n",
       " 836: 'cuisine',\n",
       " 837: 'mest',\n",
       " 838: 'anniversaire',\n",
       " 839: 'sol',\n",
       " 840: 'fou',\n",
       " 841: 'font',\n",
       " 842: 'lumière',\n",
       " 843: 'certains',\n",
       " 844: 'attendu',\n",
       " 845: 'tour',\n",
       " 846: 'résoudre',\n",
       " 847: 'lhistoire',\n",
       " 848: 'sac',\n",
       " 849: 'rentré',\n",
       " 850: 'hors',\n",
       " 851: 'mit',\n",
       " 852: 'courses',\n",
       " 853: 'bons',\n",
       " 854: 'docteur',\n",
       " 855: 'doute',\n",
       " 856: 'amusant',\n",
       " 857: 'lhôpital',\n",
       " 858: 'daide',\n",
       " 859: 'prudent',\n",
       " 860: 'ordinateur',\n",
       " 861: 'travers',\n",
       " 862: 'voulu',\n",
       " 863: 'rappeler',\n",
       " 864: 'finir',\n",
       " 865: 'certaines',\n",
       " 866: 'aurais',\n",
       " 867: 'restaurant',\n",
       " 868: 'arrêtez',\n",
       " 869: 'savez',\n",
       " 870: 'faistu',\n",
       " 871: 'amoureux',\n",
       " 872: 'sourire',\n",
       " 873: 'jeu',\n",
       " 874: 'réparer',\n",
       " 875: 'carte',\n",
       " 876: 'suffisamment',\n",
       " 877: 'préfère',\n",
       " 878: 'tombé',\n",
       " 879: 'ouvert',\n",
       " 880: 'faute',\n",
       " 881: 'chats',\n",
       " 882: 'mourir',\n",
       " 883: 'prêter',\n",
       " 884: 'soin',\n",
       " 885: 'semblait',\n",
       " 886: 'fatiguée',\n",
       " 887: 'droite',\n",
       " 888: 'souci',\n",
       " 889: 'eux',\n",
       " 890: 'langues',\n",
       " 891: 'gentil',\n",
       " 892: 'emploi',\n",
       " 893: 'musée',\n",
       " 894: 'intéressant',\n",
       " 895: 'linstant',\n",
       " 896: 'bonnes',\n",
       " 897: 'tavoir',\n",
       " 898: 'décrire',\n",
       " 899: 'pire',\n",
       " 900: 'école',\n",
       " 901: 'lever',\n",
       " 902: 'crains',\n",
       " 903: 'appeler',\n",
       " 904: 'court',\n",
       " 905: 'service',\n",
       " 906: 'voudraistu',\n",
       " 907: 'cartes',\n",
       " 908: 'bibliothèque',\n",
       " 909: 'devraisje',\n",
       " 910: 'allait',\n",
       " 911: 'faitesvous',\n",
       " 912: 'serez',\n",
       " 913: 'rarement',\n",
       " 914: 'suivre',\n",
       " 915: 'réfléchir',\n",
       " 916: 'demanda',\n",
       " 917: 'coûte',\n",
       " 918: 'retourner',\n",
       " 919: 'rire',\n",
       " 920: 'enfin',\n",
       " 921: 'debout',\n",
       " 922: 'mur',\n",
       " 923: 'piano',\n",
       " 924: 'propres',\n",
       " 925: 'courant',\n",
       " 926: 'rhume',\n",
       " 927: 'pieds',\n",
       " 928: 'invité',\n",
       " 929: 'aurait',\n",
       " 930: 'hommes',\n",
       " 931: 'moyen',\n",
       " 932: 'garçons',\n",
       " 933: 'viande',\n",
       " 934: 'radio',\n",
       " 935: 'lhabitude',\n",
       " 936: 'certaine',\n",
       " 937: 'tué',\n",
       " 938: 'signifie',\n",
       " 939: 'mien',\n",
       " 940: 'bateau',\n",
       " 941: 'prison',\n",
       " 942: 'étudiant',\n",
       " 943: 'cassé',\n",
       " 944: 'voici',\n",
       " 945: 'certain',\n",
       " 946: 'œil',\n",
       " 947: 'cheval',\n",
       " 948: 'différence',\n",
       " 949: 'pleurer',\n",
       " 950: 'intéressé',\n",
       " 951: 'ferme',\n",
       " 952: 'aies',\n",
       " 953: 'autorisé',\n",
       " 954: 'suggère',\n",
       " 955: 'lac',\n",
       " 956: 'loi',\n",
       " 957: 'sommeil',\n",
       " 958: 'japonais',\n",
       " 959: 'sérieusement',\n",
       " 960: 'dort',\n",
       " 961: 'présent',\n",
       " 962: 'compris',\n",
       " 963: 'menti',\n",
       " 964: 'présenter',\n",
       " 965: 'moindre',\n",
       " 966: 'taille',\n",
       " 967: 'libre',\n",
       " 968: 'parlez',\n",
       " 969: 'poste',\n",
       " 970: 'repas',\n",
       " 971: 'cher',\n",
       " 972: 'noir',\n",
       " 973: 'étrange',\n",
       " 974: 'las',\n",
       " 975: 'glace',\n",
       " 976: 'conseils',\n",
       " 977: 'sors',\n",
       " 978: 'papier',\n",
       " 979: 'passée',\n",
       " 980: 'quels',\n",
       " 981: 'opinion',\n",
       " 982: 'jambe',\n",
       " 983: 'supporter',\n",
       " 984: 'ditesmoi',\n",
       " 985: 'lunettes',\n",
       " 986: 'manque',\n",
       " 987: 'allezvous',\n",
       " 988: 'né',\n",
       " 989: 'tempête',\n",
       " 990: 'promets',\n",
       " 991: 'bizarre',\n",
       " 992: 'surprise',\n",
       " 993: 'facilement',\n",
       " 994: 'prévu',\n",
       " 995: 'tombée',\n",
       " 996: 'impossible',\n",
       " 997: 'restez',\n",
       " 998: 'sy',\n",
       " 999: 'douche',\n",
       " 1000: 'projet',\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc449713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 디코딩\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = fra_to_idx['<s>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx_to_fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<e>' or\n",
    "           len(decoded_sentence) > 20):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7b01c",
   "metadata": {},
   "source": [
    "번역한 문장을 테스트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b4398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f36e0407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: just go in and tell the boss you want a raise thats easier said than done\n",
      "정답 문장:  «entre seulement et dis au patron que tu veux une augmentation» «plus facile à dire quà faire» \n",
      "번역기가 번역한 문장:  nécoutes nécoutes sérieusem\n",
      "-----------------------------------\n",
      "입력 문장: i think tom may be dead\n",
      "정답 문장:  je crois que tom est peutêtre mort \n",
      "번역기가 번역한 문장:  herbert oiseau fo\n",
      "-----------------------------------\n",
      "입력 문장: can i see you in my office\n",
      "정답 문장:  puisje vous voir dans mon bureau  \n",
      "번역기가 번역한 문장:  limage montagne corall\n",
      "-----------------------------------\n",
      "입력 문장: your garden needs some attention\n",
      "정답 문장:  votre jardin a besoin dun peu dattention \n",
      "번역기가 번역한 문장:  lextirpa jadmire criti\n",
      "-----------------------------------\n",
      "입력 문장: if you have something to say just say it\n",
      "정답 문장:  si tu as quelque chose à dire disle \n",
      "번역기가 번역한 문장:  folie fascinée secréta\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][3:len(lines.fra[seq_index])-3]) \n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262edfa4",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333ec4b",
   "metadata": {},
   "source": [
    " 번역의 질이 너무 별로였다.  \n",
    " 추후 코드를 꼭 보완하고 싶다.\n",
    " \n",
    " - 토큰, 정제의 순서에 따라 계속 단어 딕셔너리가 제대로 만들어지지 않는 점.\n",
    " \n",
    " - 섞는 방법의 문제. 엉망으로 섞여버린 것 같다. 단어사전이 서로 인덱스가 매칭이 되지 않아 영 이상한 문장이 출력된 것 같다.\n",
    " \n",
    " - seq2seq 모델의 성능을 높이기 위한 하이퍼파라미터 조정. 시도 자체가 부족했다.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d489d64",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995f442",
   "metadata": {},
   "source": [
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html (seq2seq 모델링)  \n",
    "https://wikidocs.net/33793 (임베딩레이어 사용하기)  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer (케라스 제공의 텍스트 토크나이저)  \n",
    "https://www.techiedelight.com/ko/remove-punctuations-string-python/ (구두점 제거)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
